# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по лабораторной работе #5 выполнил(а):
- Ребак Анна Дмитриевна 
- РИ-220931
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Найдите внутри C# скрипта “коэффициент корреляции ” и сделать выводы о том, как он влияет на обучение модели.
- Задание 2.
- Изменить параметры файла yaml-агента и определить какие параметры и как влияют на обучение модели. Привести описание не менее трех параметров.
- Задание 3.
- Приведите примеры, для каких игровых задачи и ситуаций могут использоваться примеры 1 и 2 с ML-Agent’ом. В каких случаях проще использовать ML-агент, а не писать программную реализацию решения?
- Выводы.

## Цель работы
Познакомиться с программными средствами для создания системы машинного обучения и ее интеграции в Unity.

## Задание 1
### Найдите внутри C# скрипта “коэффициент корреляции ” и сделать выводы о том, как он влияет на обучение модели.
Ход работы:

Скрипт:
```C#

using System.Collections;
using System.Collections.Generic;
using UnityEngine;
using Unity.MLAgents;
using Unity.MLAgents.Sensors;
using Unity.MLAgents.Actuators;

public class RollerAgent : Agent
{
    Rigidbody rBody;
    // Start is called before the first frame update
    void Start()
    {
        rBody = GetComponent<Rigidbody>();
    }

    public Transform Target;
    public override void OnEpisodeBegin()
    {
        if (this.transform.localPosition.y < 0)
        {
            this.rBody.angularVelocity = Vector3.zero;
            this.rBody.velocity = Vector3.zero;
            this.transform.localPosition = new Vector3(0, 0.5f, 0);
        }

        Target.localPosition = new Vector3(Random.value * 8-4, 0.5f, Random.value * 8-4);
    }
    public override void CollectObservations(VectorSensor sensor)
    {
        sensor.AddObservation(Target.localPosition);
        sensor.AddObservation(this.transform.localPosition);
        sensor.AddObservation(rBody.velocity.x);
        sensor.AddObservation(rBody.velocity.z);
    }
    public float forceMultiplier = 10;
    public override void OnActionReceived(ActionBuffers actionBuffers)
    {
        Vector3 controlSignal = Vector3.zero;
        controlSignal.x = actionBuffers.ContinuousActions[0];
        controlSignal.z = actionBuffers.ContinuousActions[1];
        rBody.AddForce(controlSignal * forceMultiplier);

        float distanceToTarget = Vector3.Distance(this.transform.localPosition, Target.localPosition);

        if(distanceToTarget < 1.42f)
        {
            SetReward(1.0f);
            EndEpisode();
        }
        else if (this.transform.localPosition.y < 0)
        {
            EndEpisode();
        }
    }
}

```

Коэффициент корреляции встречается в методе OnActionReceived.

Коэффициент корреляции способствует определению расстояния, на котором должен находится объект, для того, чтобы агент его "увидел".
=> чем больше этот коэффициент, тем проще будет найти объект, чем он меньше - тем сложнее. Стоит учесть что при сильном уменьшении данного параметра может сильно увеличиться время обучения модели.



## Задание 2
### Изменить параметры файла yaml-агента и определить какие параметры и как влияют на обучение модели. Привести описание не менее трех параметров.
Ход работы:

Код внутри нашего .yaml вайла:
```
behaviors:
  RollerBall:
    trainer_type: ppo
    hyperparameters:
      batch_size: 10
      buffer_size: 100
      learning_rate: 3.0e-4
      beta: 5.0e-4
      epsilon: 0.2
      lambd: 0.99
      num_epoch: 3
      learning_rate_schedule: linear
    network_settings:
      normalize: false
      hidden_units: 128
      num_layers: 2
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    max_steps: 500000
    time_horizon: 64
    summary_freq: 10000

```
![image](https://github.com/Annette04/Data-analysis-and-artificial-intelligence/assets/128488854/bc525a13-201b-4feb-8d27-b9300afd74bd)

### Описание параметров:

- buffer_size: 100
Количество опыта, который необходимо собрать перед обновлением модели политики. Сколько опыта должно быть собрано, прежде чем мы приступим к какому-либо изучению или обновлению модели. Больший размер буфера соответствует более стабильным обновлениям обучения. Эта переменная должна быть в несколько раз больше, чем batch_size.

- batch_size: 10
Параметр определяет велечину, которая используется для одного обновления модели. Переменная должна быть в несколько раз меньше buffer_size.

- learning_rate: 3.0e-4
Начальная скорость обучения для градиентного спуска. Соответствует силе каждого шага обновления градиентного спуска. Обычно, если тренировка нестабильна, а вознаграждение постоянно не увеличивается, это значение следует уменьшить. У нас стоит значение по умолчанию.

- num_epoch: 3
Количество проходов через буфер опыта при выполнении оптимизации градиентного спуска. Чем больше размер пакета, тем большее колличество раз допустимо это сделать. Уменьшение этого параметра может обеспечить более стабильные обновления в связи с замедлением обучения.

- time_horizon: 64
Сколько шагов опыта нужно собрать для того, чтобы добавить агента в буфер опыта. Когда предел достигается до окончания эпизода, оценка стоимости используется для прогнозирования общего ожидаемого вознаграждения от текущего состояния агента. Этот параметр балансирует между менее предвзятой, но более высокой оценкой дисперсии и более предвзятой, но менее разнообразной оценкой.

## Выводы после имзенения параметров:
### Learning_rate
- Слишком маленькое значение сделало обучение устойчивым, но при этом сильно замедлило обучение, а слишком большое значение сделало обучение неравномерным.
### batch_size
- Уменьшив значение до 4, обучение стало проводится менее стабильно.
### num_epoch
- увелечение параметра улучшило точность обучения, но при этом сильно увеличилось время обучения.


## Задание 3
### Приведите примеры, для каких игровых задачи и ситуаций могут использоваться примеры 1 и 2 с ML-Agent’ом. В каких случаях проще использовать ML-агент, а не писать программную реализацию решения?
Ход работы:

1) ML-Agent’а можно использовать в такой игре как PackMan. Для того чтобы враги обучались быстрее и точнее находить игрока на карте, а также высчитывать оптимальную скорость движения и маршрут для его поимки.
2) В играх - симуляторах, можно обучать персонажей выполнять ту или иную часть работы, чтобы оптимизировать рабочий процесс на своих производствах.
3) Если обучаемой модели планируеться дать большой пакет задачь, которым она должна обучиться в короткий срок, то в таком случае целесообразнее прописать конкретную инструкцию (ход действий), а не использовать ML-Adent.

## Выводы
- В ходе лабораторной работы были изучены средства для создания системы машинного обучения и ее интеграции в Unity. Проведены эксперименты с параметрами в .yaml файле. Выявлено, как параметры могут влиять на тренировку модели. А также смоделированны ситуации в которых можно использовать ML-Agent, а в которых проще написать игровую логику для конкретного поведения персонажа.


| Plugin | README |
| ------ | ------ |
| GitHub | [plugins/github/README.md][PlGh] |
| Visual Studio| [plugins/visualstudio/README.md][PlGh] |
| Unity | [plugins/unity/README.md][PlMe] |
| Pycharm | [plugins/pycharm/README.md][PlGa] |
| Anaconda Prompt | [plugins/anacondaprompt/README.md][PlGa] |
| TensorBoard | [plugins/tensorboard/README.md][PlGa] |

## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**

